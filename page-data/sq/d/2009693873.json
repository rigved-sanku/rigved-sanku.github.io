{"data":{"jobs":{"edges":[{"node":{"frontmatter":{"title":"ADAS Computer Vision Intern","company":"Honda Research Institute (HRI)","location":"San Jose, CA","range":"Jan 2025 – Present","url":"https://www.honda-ri.com/"},"html":"<ul>\n<li>Built a <strong>real-time driver-intent prediction</strong> pipeline that fuses 2-D / 3-D body-pose and head-pose cues, achieving <strong>95 % accuracy</strong> with end-to-end latency under <strong>30 ms</strong>.</li>\n<li>Ported the models to <strong>TensorRT FP16</strong> with ONNX graph cleanup, delivering a <strong>3 × speed-up</strong> and <strong>30 % GPU-memory reduction</strong> for edge deployment.</li>\n<li>Filed provisional U.S. patent <strong>#63/782 414</strong> covering the multimodal vision architecture for intent prediction.</li>\n</ul>"}},{"node":{"frontmatter":{"title":"Computer Vision Research Assistant","company":"VIS Lab, Robotics Dept, WPI","location":"Worcester, MA","range":"Aug 2024 – Jan 2025","url":"https://www.wpi.edu/academics/departments/robotics-engineering"},"html":"<ul>\n<li>Designed a <strong>multimodal framework</strong> for 3D LiDAR point cloud shape completion, fusing 3D geometric features with 2D image cues to improve object <strong>reconstruction</strong> across diverse datasets; work submitted to <strong>IROS 2025</strong> [<a href=\"https://drive.google.com/file/d/1qrxQeTK-QDyAH1fRY2KX12Gec9ivh7yh/view\">Link</a>] [<a href=\"https://drive.google.com/file/d/1N6xhVy37BpSJRCnh7xuHdNrAbjghTz51/view\">Poster</a>].</li>\n<li>Engineered a novel <strong>multi-level cross-attention mechanism</strong> combining PointNet++ and ResNet-18 features for effective multimodal fusion in point cloud completion tasks.</li>\n</ul>"}},{"node":{"frontmatter":{"title":"Computer Vision Intern","company":"Emerging Technology, MUSCO","location":"West Des Moines, IA","range":"May 2024 – Aug 2024","url":"https://www.musco.com/"},"html":"<ul>\n<li><strong>Designed a full crowd-monitoring pipeline</strong> for stadium entry/exit flows using YOLO-v5 for detection and <strong>ByteTrack + SORT</strong> for multi-object tracking, improving counting accuracy by <strong>15 %</strong> over the previous system.</li>\n<li>Built and trained a <strong>custom person-re-identification model</strong> (ResNet-50 backbone with ArcFace loss) that reached <strong>98.1 % Rank-1</strong> and ran in real time on an NVIDIA Jetson Xavier NX.</li>\n<li>Automated dataset generation &#x26; augmentation scripts, boosting mean Average Precision by <strong>10 %</strong> after re-training with synthetic viewpoints and lighting.</li>\n<li>Packaged the pipeline into Docker containers with CI tests, enabling friction-less deployment for field engineers.</li>\n</ul>"}},{"node":{"frontmatter":{"title":"Computer Vision Research Assistant","company":"Computer Science Dept, WPI","location":"Worcester, MA","range":"Sept 2023 – Jan 2024","url":"https://www.wpi.edu/academics/departments/computer-science"},"html":"<ul>\n<li>Designed a novel <strong>transformer-based architecture</strong> to improve <strong>object detection</strong> in adverse weather conditions for autonomous vehicles; work submitted to <strong>ICCV 2025</strong> [<a href=\"https://drive.google.com/file/d/1oHou6dvBSZXjW5Z-boLfd3GG388aZdY_/view\">Link</a>].</li>\n<li>Engineered <strong>information mixing</strong> between adverse and rich visual embeddings by integrating <strong>SAM features</strong> with <strong>Faster R-CNN</strong>, and performed detailed ablation studies to optimize real-world <strong>ADAS</strong> performance.</li>\n</ul>"}},{"node":{"frontmatter":{"title":"Computer Vision Intern","company":"Indian Space Research Org (ISRO)","location":"Bengaluru, India","range":"Dec 2022 – May 2023","url":"https://www.isro.gov.in/"},"html":"<ul>\n<li><strong>Revamped a real-time human-action prediction pipeline</strong> for mission-control video streams, boosting top-1 accuracy from <strong>74 % → 96 %</strong> by integrating <strong>OpenPose</strong> key-points with temporal smoothing.</li>\n<li>Designed and trained a <strong>Bi-Directional LSTM with attention + ensemble voting</strong>, delivering state-of-the-art performance at <strong>20 fps</strong> on NVIDIA T4 GPUs.</li>\n<li>Co-authored a peer-reviewed paper in <em>Springer Signal, Image &#x26; Video Processing</em> (2024) and open-sourced the dataset for reproducibility.</li>\n<li>Collaborated with robotics and payload teams to test under dynamic lighting and telemetry noise, reducing false positives by <strong>35 %</strong> in field trials.</li>\n</ul>"}},{"node":{"frontmatter":{"title":"Deep Learning Intern","company":"Dept. of PwD, Govt. of India","location":"New Delhi, India","range":"Dec 2021 – May 2022","url":"https://disabilityaffairs.gov.in/"},"html":"<ul>\n<li><strong>Engineered an audio-guided navigation assistant</strong> for the visually impaired, fusing Vision Transformers with LiDAR depth to detect obstacles and announce safe walking paths in real time.</li>\n<li>Designed an <strong>IMU-based fall-detection module</strong> that combined accelerometer and gyroscope data through sensor-fusion filters, reaching a <strong>94 % detection rate</strong> with &#x3C; 200 ms latency on ARM devices.</li>\n<li>Built a multimodal Vision–Language pipeline (VQA + image captioning) using BERT encoders and LLM decoders, achieving <strong>85 % accuracy</strong> while remaining lightweight enough for Jetson-class edge hardware.</li>\n<li>Delivered Dockerized prototypes, documentation, and a recorded demo to rehabilitation-center partners, enabling rapid adoption in assistive-technology trials.</li>\n</ul>"}},{"node":{"frontmatter":{"title":"Computer Vision Intern","company":"Carnot Technologies","location":"Mumbai, India","range":"Jun 2021 – Sep 2021","url":"https://carnot.com/"},"html":"<ul>\n<li>Built a <strong>U-Net + ResNet-34</strong> image-segmentation model that predicts field patches from tractor sensor data on a lat-long grid, achieving <strong>90 % test-recall</strong>.</li>\n<li>Applied <strong>DB-SCAN clustering</strong> to pattern-recognise and differentiate farm patches, boosting feature-assessment scores to <strong>87–100 %</strong> across varied datasets.</li>\n</ul>"}}]}}}